%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    PAPER for MA 952/672
%    Tony Zhang
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{article}

\usepackage{tony}

\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}


\DeclareMathOperator{\softmax}{softmax}

\title{Voice Classification by Neural Networks}
\author{Tony Zhang}

\begin{document}
\maketitle

\begin{abstract}
% abstract
\end{abstract}

\section{Introduction}

% introduce things
% theory background

Though still a relatively new field, machine learning, the study of algorithms that can ``learn" dynamically, holds much potential. 
% blah blah blah

A curious development in the field was the \emph{neural network}, a biologically inspired machine learning model. Indeed, we see how the nodes in \cref{fig:neuralnet} resemble neurons sending information to other neurons. We shall soon make these concepts mathematically rigorous.

The goal of a neural network is to classify a \emph{feature vector}; that is, given some input data (``features", e.g. the batting average of a particular baseball player, the temperature that particular day), we would like to place this data point into one of a number of \emph{classes} (e.g. whether his/her next swing will/will not be a strike). In particular, the neural network will be a function taking input data and outputting some numbers that can be interpreted as the probability that this data point is in a particular class.\footnote{To be nitpicky, we should only say that these numbers are indices for the model's confidence that the data point is in some class.}

Let's make that concrete. For our baseball example, suppose we'd like to predict whether Lauren the baseball player (batting average 0.315) will hit the ball on her next swing. The current temperature at the stadium is 21.6 degrees Celsius. So we give the neural net the feature vector $(0.315, 21.6)$. The neural net then outputs us $(0.21, 0.79)$---a strike with ``probability" 0.21 and a non-strike with ``probability" 0.79.

% picture of neural net
\begin{figure}[htbp]
\centering
\input{aux/neuralnet.tex}
\caption{A simple neural network.}
\label{fig:neuralnet}
\end{figure}

\subsection{Neural Net Theory}

How does a neural network make these predictions?
Each neural network takes the input and sends it through a set of \emph{neurons} (each carrying a value) organized into \emph{layers}.
(\cref{fig:neuralnet} displays 3 layers, but a neural net can have arbitrarily many layers, with arbitrarily many neurons in each.)
The value of each neuron is either predetermined (as are the ``bias" neurons in \cref{fig:neuralnet}, whose values are fixed at 1, or the input neurons, whose values are just the individual components of the feature vectors---one input neuron could contain a baseball player's batting average, for example) or computed from the values of neurons in the previous layer.

For our purposes, each neuron $z$ (except for the bias neuron) in a \emph{hidden layer} (layer that isn't the first or last) depends on the neurons $x_i$ in the previous layer as follows:
\[ z = \sigma\left(\sum_i \alpha_i x_i \right) \]
where $\sigma(t) = (1 + \exp(-t))^{-1}$ is the logistic function and the $\alpha_i$ are some known constants (imagine these as unique ``weights" attached to each arrow between neurons in \cref{fig:neuralnet}). We shall see later where the values of $\alpha$ come from.

The values of the neurons $y_k$ ($k = 1, \dots, K$) in the output layer, however, are defined as follows:
\[ y_k = \softmax_k(n_1, n_2, \dots, n_K) = \frac{\exp(n_k)}{\sum_j \exp(n_j)} \]
where each $n_k$ is a ``proto-value" associated with neuron $y_k$ with value
\[ n_k = \sum_i \beta_i x_i \]
where the $\beta_i$ are known constants (like the $\alpha_i$ from a little before) and the $x_i$ are the neurons in the penultimate layer.

In general, with the exception of the input neurons and the bias neurons, each neuron's value is simply produced by applying some nonlinear function (e.g. $\sigma, \softmax$) to a linear combination of the values of the neurons in the previous layer. The weights on the linear combination are predetermined. Clearly, to teach a neural net, we must figure out the right weights for the situation.

\subsection{Training}




\section{Methodology}

    % number of data points
% feature vectors (and which ultimately chosen)
    % structure of neural net
    % gradient descent

\section{Results}

    % ROC AUC scores and such

\end{document}
